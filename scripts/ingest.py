import os
import sys
import shutil
import chromadb
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# å°† backend è·¯å¾„æ·»åŠ åˆ° sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend', 'app'))
from core.config import settings

KNOWLEDGE_BASE_DIR = "knowledge_base"
COLLECTION_NAME = "interview_assistant"

def clean_chroma_data(chroma_data_path):
    """
    å®Œå…¨æ¸…ç†ChromaDBæ•°æ®ç›®å½•
    """
    if os.path.exists(chroma_data_path):
        print(f"ğŸ—‘ï¸ æ¸…ç†ç°æœ‰ ChromaDB æ•°æ®ç›®å½•: {chroma_data_path}")
        try:
            shutil.rmtree(chroma_data_path)
            print("âœ… æˆåŠŸæ¸…ç†æ—§æ•°æ®")
        except Exception as e:
            print(f"âš ï¸ è­¦å‘Š: æ— æ³•æ¸…ç†æ—§æ•°æ®: {e}")
    
    # é‡æ–°åˆ›å»ºç›®å½•
    os.makedirs(chroma_data_path, exist_ok=True)

def main():
    print("ğŸš€ å¼€å§‹çŸ¥è¯†åº“æ‘„å–...")

    # å®šä¹‰ ChromaDB æ•°æ®å­˜å‚¨è·¯å¾„ï¼Œä¸ rag_service.py ä¸­çš„è·¯å¾„ä¸€è‡´
    chroma_data_path = os.path.join(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')), "chroma_data")
    
    # å®Œå…¨æ¸…ç†æ•°æ®ç›®å½•ï¼ˆæ¨èç”¨äºå¼€å‘é˜¶æ®µï¼‰
    clean_chroma_data(chroma_data_path)
    
    print(f"æ‘„å–è„šæœ¬ä½¿ç”¨ ChromaDB (æŒä¹…åŒ–å®¢æˆ·ç«¯) è·¯å¾„: {chroma_data_path}")
    # åˆå§‹åŒ– ChromaDB çš„æŒä¹…åŒ–å®¢æˆ·ç«¯
    client = chromadb.PersistentClient(path=chroma_data_path)

    # 1. åŠ è½½æ–‡æ¡£
    all_documents = []

    try:
        pdf_loader = DirectoryLoader(
            KNOWLEDGE_BASE_DIR,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True,
            use_multithreading=True
        )
        all_documents.extend(pdf_loader.load())
        print(f"ğŸ“• åŠ è½½äº† {len(all_documents)} PDF æ–‡æ¡£") # ä¿®æ­£è®¡æ•°
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ PDF æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    try:
        text_loader = DirectoryLoader(
            KNOWLEDGE_BASE_DIR,
            glob="**/*.txt",
            loader_cls=TextLoader,
            show_progress=True,
            use_multithreading=True
        )
        txt_docs = text_loader.load()
        all_documents.extend(txt_docs)
        print(f"ğŸ“„ åŠ è½½äº† {len(txt_docs)} TXT æ–‡æ¡£")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ TXT æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    try:
        docx_loader = DirectoryLoader(
            KNOWLEDGE_BASE_DIR,
            glob="**/*.docx",
            loader_cls=Docx2txtLoader,
            show_progress=True,
            use_multithreading=True
        )
        docx_docs = docx_loader.load()
        all_documents.extend(docx_docs)
        print(f"ğŸ“˜ åŠ è½½äº† {len(docx_docs)} DOCX æ–‡æ¡£")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ DOCX æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    documents = all_documents

    if not documents:
        print(f"âŒ åœ¨ {KNOWLEDGE_BASE_DIR} ç›®å½•ä¸­æœªæ‰¾åˆ°æ–‡æ¡£ã€‚ä¸­æ­¢ã€‚")
        return

    print(f"ğŸ“š æ€»å…±åŠ è½½çš„æ–‡æ¡£æ•°é‡: {len(documents)}")

    # 2. å°†æ–‡æ¡£åˆ†å‰²æˆå—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ğŸ“„ åˆ†å‰²æˆ {len(chunks)} ä¸ªå—ã€‚")

    # 3. åˆå§‹åŒ– LangChain çš„ HuggingFace Embeddings
    print("ğŸ§  ä½¿ç”¨ LangChain HuggingFaceEmbeddings åˆ›å»ºåµŒå…¥...")
    embeddings_for_langchain_chroma = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 4. å­˜å‚¨åˆ° ChromaDB (ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯)
    print(f"ğŸ’¾ å°†å—å­˜å‚¨åˆ° ChromaDB é›†åˆ: {COLLECTION_NAME}...")

    try:
        # ä½¿ç”¨ LangChain çš„ Chroma.from_documents æ¥åˆ›å»ºå¹¶å¡«å……å‘é‡å­˜å‚¨
        vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings_for_langchain_chroma,
            collection_name=COLLECTION_NAME,
            client=client,
            persist_directory=chroma_data_path
        )

        # è·å–é›†åˆçš„å®é™…æ–‡æ¡£æ•°é‡è¿›è¡Œç¡®è®¤
        document_count = vector_store._collection.count()
        print(f"âœ… æ‘„å–å®Œæˆ! é›†åˆä¸­çš„æ€»æ–‡æ¡£æ•°é‡: {document_count}")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if document_count != len(chunks):
            print(f"âš ï¸ è­¦å‘Š: é¢„æœŸ {len(chunks)} ä¸ªæ–‡æ¡£ä½†æ‰¾åˆ° {document_count} ä¸ª")
        
    except Exception as e:
        print(f"âŒ æ‘„å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()
